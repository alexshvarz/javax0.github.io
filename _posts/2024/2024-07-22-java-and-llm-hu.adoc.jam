---
{%@define title=Mi az LLM és a RAG?%}\
title: {%title%}
layout: posta
comments: on
---
{%@import javax0.jim%}
{%@comment

=========================================================================================
%}{%@sep 〔〕%}

= 〔title〕

〔section Bevezető〕

Mi ez a nagy Artificial Intelligence őrület? És mi ez az RAG, amit annyit emlegetnek?
A RAG az a Retrieval Augmented Generation-nek a rövidítése.
Ez egy angol betű szó.
És igazából ez az a technológia, amivel megpróbálják kiegészíteni az LLM, azaz a Large Language Model alkalmazásokat úgy, hogy azok képesek legyenek kezelni olyan tudásbázist, olyan tudást, amely a Large Language Modelben magában nem található meg.
A Large Language Model önmagában egy olyan neurális hálózat, amit valamilyen módon tréningeztek, valamilyen adatokkal, általában olyan adatokkal, amelyek szabadon rendelkezésre állnak az interneten, igen nagy mennyiségű adattal, aminek hatására a Large Language Modelek szinte emberi módon képesek kommunikálni, ha valamilyen kérdést felteszünk nekik, akkor erre tudnak válaszolni, de fogalmuk nincsen róla, hogy milyen adatok vannak, milyen információk vannak egy vállalaton belül, hiszen ezek nem publikus adatok.
Az elelem nem is tanítható meg a mostani formában ezekre az információkra, mert azok az alkalmazások, amelyek most rendelkezésre állnak, azok úgy működnek, hogy kialakítanak a fejlesztők valamilyen modellt, utána trénelik ezt a modellt, több millió vagy milliárd paramétert a tréning adatokkal beállítanak, kérdéseket tesznek fel neki, jönnek válaszok, és akkor utána annak megfelelően, hogy a válasz mennyire jó vagy nem jó, állítgatják ezeket a paramétereket.
Természetesen nem kézzel-elre vannak programok és algoritmusok.
De amikor ez készen van, és ez néhány hónap alatt készül el a mostani viszonyok között, egy kisebb városenergia igényét felhasználva, sok ezer gépen futtatva ezt a tuningoló algoritmust, akkor utána ez a modell, ami egyébként letölthető, futtható egy saját gépen, ez 1-2 GB-i adatot jelent maga a modell, akkor utána ez a neurális hálózat már nem változik, nem tanul meg új dolgokat.
Akkor tud új dolgot megtanulni, ha kapunk belőle egy új változatot.
Mégis szeretnénk ezt a fajta lehetőséget egy cégen belül használni, mégpedig úgy, hogyha egy kérdést fölteszünk ennek a neurális hálózatnak, tehát ennek az ellenem modellnek, akkor szeretnénk, hogyha olyan választ adna, ami figyelembe veszi a mi saját cégünk belső információit.
Olyan tudjuk ezt megtenni.
Úgy tudjuk megtenni, mintha egy emberrel is ilyesmit csinálnák, hogy jön valaki a céghez, kérdéseket akarunk neki föltenni a cégkel kapcsolatban, de nem tud semmit a cégünkről, akkor először megtanítjuk dolgokra, odadunk neki információkat.
És akkor ő ezeket az információkat el fogja tenni a saját neurális hálójába.
De gondolhatjuk azt, hogy fókuszálva a munkára, amikor haza megy minden mást elfelejtett, hogy ezeket az információkat, amik cégspecifikusak, ezeket egy külön helyre rakja el.
És az elelemnél, illetve az erágénál is ez a modell, hogy azokat az információkat, amik nincsenek benne az elelem neurális hálójában, azokat külön rakjuk el egy külön adatbázisba.
Ha másért nem, akkor azért, mert magának a neurális hálónak az adatbázisába a modelljében nem tudjuk beletenni, miután ezek nem publikusak, hogy hogyan néznek ki, hogyan épülnek fel, nem is feltétlenül módosíthatók már abban a formában, ahogyan a programban benne vannak.
Nincsen meg, mondjuk talán így, hogy a forráskódja az adatoknak, nem feltétlenül a program forráskódja, hanem az adatok eredeti formája.
Ez a modell, ez több lépcsőn keresztül lesz végül is egy 1 GB-os, tehát aránylag kicsinek tekinthető, relatív, hogy mi a kicsi, de egy olyan az értelemben, ez kicsinek tekinthető adathalmaz.
És nem biztos, hogy ez még olyan állapotban van, ami módosítható.
Ha egy külön adatbázisba akarjuk betenni azokat az információkat, amik saját információk, akkor erre úgynevezett vektoradatbázist szokás használni.
A vektoradatbázis pedig egy speciális olyan alkalmazás, ami két szövegdarabról meg tudja mondani, hogy mennyire vannak közel egymáshoz.
Tehát mennyire szólnak ugyanarról.
Mennyire ugyanazok a kulcs szabak benne.
A tudásbázist, ami a cégen belül rendelkezés nyaral, felszabdaljuk szövegdarabokra.
Ezek a szövegdarabok tipikusan ilyen ezer karakter, ezer betű hosszúságúak, és ezek képeznek egy-egy rekordot.
Van közöttük egy kis átfedés, tehát nem ott kezdjük a körülvetkezőt, ahol az előzőnek vége szakadt, hanem egy kicsit előbb, hogy legyen egyfajta kontextus és folyamatoság a szövegben.
És minden egyes ilyen szövegdarabot elteszünk egy adatbázisba, és megkérünk egy úgynevezett embedding algoritmust, hogy a szövegdarabhoz rendeljen hozzá egy úgynevezett vektort.
A vektor az egy számsorozat.
Ez hasonló ahhoz, mint például egy GPS koordináta.
Tulajdonképpen ez a vektor, ez ennek a darab szövegnek egy térbeli koordinátája, de ez a tér nem háromdimenziós, hanem nagyon sokdimenziós.
Amikor feltesz egy kérdést a felhasználó a RAG technológiával fejlesztett alkalmazásnak, akkor ezt a kérdést is vektorizáljuk, tehát megkérjük az embeddings rendszert, hogy mondja meg, hogy a térben hol helyezkedik el ez a kérdés.
És utána a vektoradat bázistól, amiből beleraktuk az összes szövegdarabunkhoz tartozó vektorokat, meg tudjuk kérdezni, hogy melyek azok a szövegdarabok a mi tudásbázisunkból, amelyek a legközelebb vannak térben a kérdéshez.
Ez egy távolságszámolás és egyfajta indexelés.
Ha úgy tetszik, Pitagorasztét ellen lehet számolni a távolságot egy ortogonális vektor térben.
Bonyolultnak hangzik, nem is kell vele igazából foglalkozni, nem kell tudnunk, hogy ez hogyan működik.
A lényeg az, hogy ez az embedding algoritmus, ami amúgy szintén egy neurális hálón szokott alapulni.
Vannak nagyon egyszerű embedding algoritmusok is, ezek praktikusan kevésbé használható, de vannak olyan bonyolultabb neurális hálók, amik ezt megteszik, nyelvtől függőek.
A vektor adott bázis megmondja, hogy melyek azok a szövegdarabok a mi tudásbázisunkból, amelyek közel vannak a kérdéshez, vagyis relevánsak a kérdés megválaszolásához.
És ezek után az elelemtől mi egy olyan promptot kérdezünk, ami nem az eredeti prompt, hanem elé betesszük azokat a szövegdarabokat, amelyeket kiszedtünk a saját tudásbázisunkból.
Az egészet nem tehetjük oda egy kérdésbe, mert az túl sok lenne, de néhányat, ötöt, hatot, hetet, vagy akár tizet oda tudunk tenni a tudásbázisból.
Beleírjuk a promptba, hogy ez egy kontextus, és a választ ebben a kontextusban szeretnénk megkapni, majd magát a kérdést, és utána ezt elküldjük az elelem algoritmusnak, ami ezt szépen elolvassa, csinál vele valamit, megválaszolja.
És tulajdonképpen ez az egész RAG ennyire egyszerű, kell hozzá egy vektoradatbázis, föl kell darabolnunk a szöveget, ha valaki ért a programozáshoz, az tudja, hogy ez nem egy olyan nagy dolog, bele kell tenni egy normál adatbázisban magát a szöveget, hogy magát a szöveget a prompt gyártásához vissza tudjuk állítani, beletesszük a vektorokat a vektoradatbázisba, hogy meg tudjuk kérdezni, hogy melyik a releváns szövegdarabok egy adott kérdéshez, és utána egy programból föl kell tudnunk tenni a kérdést az elelemnek, sztenderd interfészeket kell tudni programozni, és utána a választ vissza kell tudni küldeni az ügyfélnek, felhasználónak, aki ezt el tudja olvasni.
És ezzel a technológiával elő tudtunk állítani egy olyan alkalmazást, amelyikkel ugyanúgy lehet csetelni, mint a chat GPT-vel, de nem csak a nagyvilág dolgait tudja egy adott időpillanatig, amikor is lezárták a tréningét, hanem tudja azokat a dolgokat is, amik a mi speciális tudásbázisunkban vannak benne.
