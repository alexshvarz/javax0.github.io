---

{%@define title=Java and LLM%}\ title: {%title%} layout: posta comments: on ---
{%@import javax0.jim%}
{%@comment

=========================================================================================
%}{%@sep 〔〕%}

= 〔title〕

〔section Introduction〕

Recently, I watched a YouTube video by the brilliant Nicolai Parlog.
He was explaining why Java could be excellent for artificial intelligence applications.
He mentioned that currently, Java doesn't handle some things like big data and large memory usage very well, which I understand.

This is why Project Valhalla is underway.
However, I strongly believe that creating an application using artificial intelligence, specifically large language models and Retrieval-Aaugmented Generation (RAG), doesn't require any specific programming language.
Java is a perfect candidate for this, even with release 21, which is currently the latest version.

I'd like to elaborate on why I think this is the case.

〔section Why Java〕
Why do I believe there's no need for Project Valhalla or any other features that make minor differences between Java and other languages to develop LLM applications?
Why Java is just as suitable as any other language for developing LLM applications.
It's important to understand that I'm not claiming Java is particularly good for writing an LLM engine that performs neural network calculations.
However, other languages like Go, Rust, or Python don't typically handle that either.

Although Python is often the first choice for LLM applications, I think to understand why Java is equally suitable, we need to examine the subtle differences between various programming languages.
We must understand how an LLM application works with RAG (Retrieval-Augmented Generation).
Then we can realize that we don't need to wait for Java to deliver future projects like Valhalla or anything else.

We can use Java today.

Admittedly, I'm a Java expert, so it's easy to assume I'm biased towards Java because that's where my expertise lies.
I've invested the last 20 years in developing Java skills.

However, I don't identify solely as a Java developer.
I consider myself more of a software developer and architect.

With over 30 years of experience predating Java's existence, I've programmed in Assembly, BASIC, COBOL, FORTRAN, and many other languages.
In this discussion, I'd like to compare Rust, Go, Java, and Python as potential languages for developing LLM applications.

〔section Why Not Python〕
Initially, I didn't consider Python for this article, despite it being the foremost programming language associated with artificial intelligence and large language models (LLMs). I believe Python isn't ideal as an application language for such applications.

Python excels in prototyping and developing applications with limited user bases, rather than enterprise-grade solutions. When the first LLM applications were developed—and to some extent still today—these applications were experimental, with people exploring their potential.

Installing Python-based applications isn't user-friendly. It requires executing PIP commands, a Python-specific tool, and having Python pre-installed. This contrasts with the simplicity of Windows installations, where users typically download an executable, run the setup, and follow a series of clicks. On Mac, it's even simpler: users open a DMG file and move the application to the Applications folder.

So why is Python so popular? Primarily because it's easy to learn for those interested in LLMs and AI development, particularly those working on neural network systems. It's a straightforward language.

I firmly believe that Python is an excellent choice for introducing programming to children in late primary or early secondary school (around ages 12-14). It's ideal for understanding basic programming concepts.

Many AI researchers are mathematicians focusing on neural networks, with programming as a secondary skill. Python appealed to them because it easily integrates with libraries written in low-level languages, typically C. Java, in comparison, is more challenging to integrate with native code.

For Java, it's often preferable to run applications in separate processes, like an SQL server, and communicate over the network—a point we'll revisit later as an optimal architectural approach.

Python was initially chosen for its ease of use and low entry barrier. However, for enterprise development, I don't consider it the best choice. There aren't many enterprise-grade applications written in Python, largely due to its nature as a scripting language. The other languages I've mentioned—Go, Rust, and Java—are more suitable for enterprise development requirements.

〔section Select Your Language〕

And if we look at the development and how we select the programming language, then actually selecting a programming language for an LLM application is not that different than selecting a programming language for any other application.

〔subsection Can it do?〕

And when we select a programming language, then we have to consider several things.
So the very first thing is the project's requirements.
Can we do that?
Can we perform and can we develop a program in the given programming language?

Some yet programming languages may not fit our needs from this point of view, like some very simple scripting languages like BASIC or Lua or something may not even be possible to do that.

〔subsection Performance, speed〕

Then we have to look at the performance requirements.
I think if we look at Python, that's a problematic thing, by the way, but most of the code in an LLM application is not running in Python, it's running in C. So even saying that Python is the programming language for LLM and neural networks is not really true because more like it is the glue language only.

And if we have an enterprise application which is using LLM and other things, then we can look at Java, performance is fairly good, Go language also fairly good, and Rust, obviously, performance unbeatable, almost unbeatable.

〔subsection Existing code-based compatibility, Developer Culture〕

Existing code-based compatibility we have to look at, but these applications are kind of fairly new, so it's more like a cultural thing within the society that is developing.
So if you are developing an application within a large organization, an in-house application, then you should pay attention what code base you already have, what knowledge you have, and what people and developers are accessible and capable of performing the task.

And yes, this is also developer availability and expertise and development speed.

These are all very similar things because if the expertise in an enterprise is mainly Java, which is very likely as opposed to Go or Rust unless you are Google, then most probably if you select Java or Kotlin, the development speed will be higher because the people know how to develop in these languages.

〔subsection Maintainability, Ecosysteme〕

Also maintainability, again, is something which is very much related to this community support and ecosystem.
This is where Rust a little bit is lagging in my opinion because it's fairly new.
Also if we look at Go, then it's not that big difference between Java and Go in this sense, but my opinion is that Java is definitely leading in community support and ecosystem, even in front of Go.

〔subsection Cost〕

Cost, well, generally these platforms are free of charge.
You can develop without paying any license fee or investing heavily into software using any of these languages if you consider the investment in hardware, then they are comparable.
That's just the same hardware you can use to develop Rust, Go or Java programming language.


〔subsection Cross-platform compatibility〕

Cross-platform compatibility, Java is practically unbeatable, but there is no big difference with Rust and Go.
They compile on the two major platforms, which is Intel and also on ARM for most of the operating systems and application, especially enterprise application.
Usually these days run on Linux systems and the compilers are available, so it's not a problem.
Application capabilities is very much like ecosystem kind of thing, but also very much depends on what kind of other systems you have in your environment.
If you have to communicate with some mainframe application or some other legacy application, what interfaces do they have?
Usually it's most probably TCP/IP that you communicate with.
In some weird applications, it may be SNA or some other old-style networks, but even in those cases, it's usually embedded already into TCP/IP and most of the services already have.
And if originally they only had SNA or some other interfaces, today they provide TCP/IP interfaces.

〔subsection Testing and debugging〕

Testing and debugging, no problem with any of these three languages.
Deployment is practically the same with the cloud operation using cloud technology.
Java is absolutely usable.
Go, again, very much usable.
Rust shouldn't really be a problem.
Library and framework availability is, again, the ecosystem.

〔subsection Vendor support〕

Vendor support, well, in that case, maybe Java is leading because there are a lot of independent software vendors who provide you with developers, so you can hire developers to program in Java.
A little bit less in Go and even less in Kotlin.

And that's the reason why Kotlin is so popular these days among developers, because if you learn Kotlin, you might very easily can find a very well-paying job if you are really good in Kotlin, not just average, really good in Kotlin.
A few years ago, like two years ago, I have seen an advertisement where they said that they are willing to pay $400,000 per year for a financial application for anyone who is ready to risk Go and go there and risk his current position and willing to learn Rust.
So they didn't even require that you already know Rust.
I don't know how real the offer really was, but I have seen the offer myself in Switzerland as an advertisement.
Documentation quality is also something you have to consider, but this is something that these three languages, Rust, Go, and Java are the same.

〔section What is an LLM application with retrieval augmented generation〕

Now let's have a look at what is an LLM application with retrieval augmented generation.
When you have an LLM system, the interface is usually Rust interface technically, but it can be anything else.
It's not really important.
It's a network interface.

〔subsection LLM〕

What you send to this interface is some human text, usually English, but it can be any other language because these LLM systems can handle different languages.
You ask a question and then the LLM, based on its database, gives you a human-like answer.
There is no magic in it so far.
What an application needs to do, what makes it special, because this is a chat application, we don't need to program it.
It's ready.
It's available.


〔subsection RAG〕

The only thing you need to program is to make it a retrieval augmented generation.
And the retrieval augmented generation makes it possible to extend the knowledge base of the LLM neural network without changing the already trained neural network.
So the LLM system has a neural network which is already trained.
And the weight and the connections in this neural network are frozen.
They don't change ever.

What we can do is add extra information to it.
And to do it, we use a technology which is so-called vector databases and embedding.
And it works using a vector database which contains all the local knowledge that we have and we can add.

So we have something separate knowledge base from the LLM which contains the neural network, and this neural network represents the language model.
So all the general knowledge which is needed for the LLM to talk to you in a human and generate a human answer.

And when you ask something from the RAG enhanced system, then the first thing before asking this question from the LLM is that the system, this application, takes this request and asks some embedding application to create a vector from it.

Now a vector technically is a series of numbers, a few hundred numbers, floating point numbers, whatever.
Think about it as an index value that you use in a database.
So it's not a record, it's the index value itself.
And in the knowledge base, in the vector database, we have all the index values of the text fragments that we have in our knowledge base.

So when we are feeding our knowledge base into the vector database, then we are splitting up the knowledge base, the sentences in our knowledge base, the documents, if you like that, into some fragments like hundred or thousand character segments.
We create a vector for each of it and we store it in the vector database, which is an index.
It's not really a whole database.
It doesn't contain the data itself.
It's only the index.

And then we have a question and we ask the vector database that here is the vector of our question.
Give me the text fragments which were indexed by this vector database during the knowledge base build up, which are the closest in this multidimensional space represented by these embedding vectors to the question.
Give me three of those.
Give me seven of those.
Not more than seven, usually.

And then we send the original prompt or the original request to the LLM and we create a prompt with a context telling the LLM that consider this context and then we put there the text for which the indexes were given us by the vector database.
So the vector database doesn't contain the text, but it says that, hey, I have these seven vectors and these seven vectors contain the location from where I can retrieve the original text or text segments.
And from these I create a prompt including the original question and the context and send it to the LLM.
And the LLM will give me the answer, and because I was giving a lot of information it, it will include this information.
So it will answer me not just based on the general knowledge, but also that if you tell me that this, this and that, then I already know I don't understand.

A human wouldn't understand why you ask this because if you give him the information, then they already, then you already know it and you can process it just as good as any other human.
But this is not a human, this is an LLM.

It can very speedily process the information you give them and it will, and it will summarize it and it will give you an answer which reflects the specialty for your knowledge base.
It doesn't read or work with the whole knowledge base.
That's the vector database that selects for you a few of those code fragments which fit most your question.
Now if I think about this as an application, what do we do in an application?
We are talking to an embedding application which is essentially a REST interface.
I give it a text, and it will give me back a JSON containing floating point numbers.
Then I'm asking a vector database, again a REST interface.

I give it a vector and it will give me seven, not the vectors because the vectors themselves are not interesting, but seven parameters, parameters for seven or so text fragments which are the closest to my vector.
Then I can retrieve from some database, again it's an Excel or something, REST or something.
I retrieve the text, then I do some text processing, creating the prompt, and then I'm talking to the LLM, sending this prompt to the LLM, getting the answer.
It's a REST interface again, and then I send it back to the user in some form.
So I'm talking to different REST services, one after the other in a sequence.
This is very typical for an enterprise-grade Java application.
If I want to be a little bit funny, then I could say that no, no, because an enterprise-grade Java application certainly should use SOAP.
But these days we are using less and less SOAP, even in the large corporations as well.
So why wouldn't Java be fit for that?
And if I compare all the other things like developer availability, ecosystem, library availability, integration with existing enterprise applications, then it has to be Java.

〔section Conclusion〕

What else?
And if you look at, for example, you compare Java with Go or Java with Rust, then if you have to manage large amount of data in memory, then definitely Go and Rust may have an advantage over Java because they have a different memory model, which I will not go into details in this talk.
But other than that, if you are using the LLM as an appliance, then Java is just as good as any other language, or even better because we usually develop enterprise applications in most of the large clients in Java.
Thank you.