---
{%@define title=Технология LLM и RAG%}\
title: {%title%}
layout: posta
comments: on
---
{%@import javax0.jim%}
{%@comment

=========================================================================================
%}{%@sep 〔〕%}

= 〔title〕

〔section Введение〕

Эта статья представляет собой краткое введение в технологию LLM и RAG.
Статья содержит много упрощений, которые могут понять обычные люди.
Если вы заинтересованы в более детальном изучении технологии, этой статьи будет недостаточно. Вы можете найти части, которые не совсем точны, хотя они хорошо передают суть.

〔section Суть технологии RAG〕

RAG - это аббревиатура от Retrieval Augmented Generation.
Это английская аббревиатура.

Эта технология пытается дополнить приложения LLM или Large Language Model.
Цель состоит в том, чтобы иметь возможность работать с базами знаний и информацией, которых нет в самой модели большого языка.
Модель большого языка - это нейронная сеть, которая была обучена каким-то образом.
Обычно их обучают на больших объемах данных, которые находятся в свободном доступе в интернете.
В результате большие языковые модели могут общаться почти как люди.
Если мы задаем им вопрос, они могут ответить на него.
Однако им нужно узнать о том, какие данные или информация существуют внутри компании, так как это не публичные данные.

〔section Ограничения LLM〕

LLM нельзя научить этой корпоративной информации в их текущем виде.
В настоящее время доступные приложения работают так, что разработчики создают некоторые модели.
Затем они обучают эту модель, устанавливая миллионы или миллиарды параметров с помощью обучающих данных.
Они задают вопросы, получают ответы, а затем корректируют эти параметры на основе качества ответов.

Конечно, для этого существуют программы и алгоритмы, а не делается вручную.
В текущих условиях этот процесс занимает несколько месяцев, используя энергию, необходимую небольшому городу.
Они запускают этот алгоритм тонкой настройки на тысячах машин.

〔section Характеристики моделей LLM〕

Когда это сделано, модель можно скачать и запустить на своей машине.
Сама модель представляет собой 1-2 ГБ данных.
После этого эта нейронная сеть больше не меняется; она не учится новому.
Она может научиться чему-то новому только если мы получим новую версию.

〔section Применение технологии RAG〕

Тем не менее, мы хотим использовать такие возможности внутри компании.
Мы хотим, чтобы эта нейронная сеть, модель LLM, давала ответ, учитывающий внутреннюю информацию нашей компании, когда мы задаем вопрос.
Мы можем сделать это так, как если бы мы делали что-то подобное с людьми.

Если кто-то приходит в компанию, и мы хотим задать им вопросы о компании, но они ничего не знают о нашей компании, мы сначала обучаем их и даем им информацию.
Они поместят эту информацию в свою нейронную сеть.

〔section Принцип работы RAG〕

Мы можем видеть, если они сосредоточены на работе и забывают все остальное, когда идут домой, и хранят эту специфическую для компании информацию в отдельном месте.

Это модель для LLM и RAG также.
Мы помещаем информацию, которой нет в нейронной сети LLM, отдельно в отдельную базу данных.
Если не по другой причине, мы не можем поместить ее в базу данных нейронной сети или в ее модель.
Это частная информация. Мы не знаем, как они выглядят или как они структурированы, и они не обязательно могут быть изменены в том виде, в котором они находятся в программе.

У нас нет, можно сказать, "исходного кода" данных - не обязательно исходного кода программы, но оригинальной формы данных.

〔section Характеристики модели LLM〕

Эта модель становится 1 ГБ через несколько шагов и представляет собой относительно небольшой набор данных.
Относительно чего это мало, но в терминах LLM это считается небольшим.
И не факт, что она все еще в состоянии, которое можно изменить.

〔section Использование векторных баз данных〕

Если мы хотим поместить свою собственную информацию в отдельную базу данных, мы обычно используем векторную базу данных.
Векторная база данных - это специальное приложение, которое может определить расстояние между двумя фрагментами текста.
То есть, насколько они об одном и том же, и сколько у них точных ключевых слов?

〔section Подготовка базы знаний〕

Мы разрезаем доступную в компании базу знаний на куски текста.
Эти куски текста обычно длиной в тысячу символов или тысячу букв и образуют отдельные записи.
Между ними есть небольшое перекрытие, поэтому мы начинаем следующий не там, где закончился предыдущий, а немного раньше.
Это для того, чтобы иметь контекст и непрерывность в тексте.

〔section Алгоритм встраивания〕

Мы помещаем каждый из этих кусков текста в базу данных и просим алгоритм встраивания присвоить ему вектор.
Вектор - это последовательность чисел.

Это похоже, например, на GPS-координаты.

По сути, этот вектор - это пространственная координата этого текста, но это пространство не трехмерное, а очень многомерное.

〔section Работа RAG для вопросов〕

Когда пользователь задает вопрос приложению, разработанному с технологией RAG, мы также векторизуем этот вопрос.

Мы просим систему встраивания сказать нам, где этот вопрос находится в пространстве.

Затем мы можем спросить у векторной базы данных, в которую мы поместили векторы, принадлежащие всем нашим кускам текста, какие куски текста из нашей базы знаний находятся ближе всего в пространстве к вопросу.

〔section Расчет расстояния между векторами〕

Это расчет расстояния и индексирование.
Если хотите, можете рассчитать расстояние по теореме Пифагора в ортогональном векторном пространстве.
Хотя это звучит сложно, нам на самом деле не нужно с этим разбираться или знать, как это работает.

〔section Характеристики алгоритма встраивания〕

Суть в том, что этот алгоритм встраивания обычно также основан на нейронной сети.
Есть и элементарные алгоритмы встраивания, но они практически менее применимы.
Существуют более сложные системы встраивания, основанные на нейронных сетях, которые делают это в зависимости от языка.

〔section Выбор релевантных кусков текста〕

Векторная база данных сообщает нам, какие куски текста из нашей базы знаний близки к вопросу, то есть релевантны для ответа на вопрос.

〔section Сборка промпта〕

После этого мы задаем LLM промпт, который не является оригинальным, а мы помещаем перед ним те куски текста, которые мы извлекли из нашей собственной базы знаний.
Мы не можем поместить все это в один вопрос, потому что это было бы слишком много, но мы можем включить несколько, пять, шесть, семь или даже десять из базы знаний.

Мы пишем в промпте, что это контекст, и мы хотим получить ответ в этом контексте, затем сам вопрос.

〔section Резюме процесса RAG〕

Затем мы отправляем это алгоритму LLM, который читает его, делает с ним что-то и отвечает на него.

И это все.
Весь RAG настолько прост.
Вам нужна векторная база данных; вам нужно разрезать текст.
Если кто-то понимает программирование, они знают, что это не большое дело.

Нам нужно поместить текст в обычную базу данных, чтобы мы могли восстановить его для генерации промпта.
Мы помещаем векторы в векторную базу данных, чтобы мы могли спросить, какие куски текста релевантны для данного вопроса.
Затем нам нужно уметь задавать вопросы LLM из программы и программировать стандартные интерфейсы.
Наконец, нам нужно уметь отправлять ответ обратно клиенту или пользователю, который может его прочитать.

〔section Заключение〕

С помощью этой технологии мы создали приложение, с которым можно общаться точно так же, как с ChatGPT.

Но оно знает не только вещи большого мира до определенного момента времени, когда его обучение было завершено, но и вещи из нашей специальной базы знаний.